<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LISTEN: Lexical vs. Acoustic Emotion Benchmark</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>
    <nav class="navbar">
        <div class="container">
            <div class="nav-brand">
                <i class="fas fa-headphones"></i> LISTEN
            </div>
            <div class="nav-links">
                <a href="#overview">Overview</a>
                <a href="#leaderboard">Leaderboard</a>
                <a href="#visualizations">Results</a>
                <a href="game.html" class="game-link">
                    <i class="fas fa-brain"></i> Human vs. AI Challenge
                </a>
                <a href="#experiments">Experiments</a>
                <a href="#examples">Examples</a>
                <a href="#citation">Citation</a>
                <a href="https://github.com/DeliJingyiC/LISTEN" target="_blank">
                    <i class="fab fa-github"></i> GitHub
                </a>
            </div>
        </div>
    </nav>

    <header class="hero">
        <div class="container hero-container">
            <div class="hero-content">
                <h1>Do Audio LLMs <span class="flip-words"><span class="flip-word">LISTEN</span><span
                            class="flip-word">Transcribe</span></span>?</h1>
                <p class="subtitle">Measuring Lexical vs. Acoustic Emotion Cues Reliance in Audio LLMs</p>
                <p class="description">
                    A comprehensive benchmark revealing that most audio-language models over-rely on text and miss
                    critical prosodic cues‚Äîespecially when words and tone conflict.
                </p>
                <div class="hero-buttons">
                    <a href="https://github.com/DeliJingyiC/LISTEN" class="btn btn-primary" target="_blank">
                        <i class="fab fa-github"></i> View on GitHub
                    </a>
                    <a href="https://huggingface.co/datasets/VibeCheck1/LISTEN_full" class="btn btn-secondary"
                        target="_blank">
                        <i class="fas fa-database"></i> Dataset
                    </a>
                    <a href="#citation" class="btn btn-secondary">
                        <i class="fas fa-quote-right"></i> Cite
                    </a>
                </div>
            </div>
            <div class="hero-visual">
                <img src="background_image4.png" alt="LISTEN Benchmark Illustration" class="hero-chart">
            </div>
        </div>
    </header>



    <section id="game-featured" class="section-featured">
        <div class="container">
            <div class="featured-game">
                <div class="featured-game-content">
                    <div class="featured-tag">üéÆ Interactive Listening Game</div>

                    <h2>Do You Listen?</h2>
                    <p class="featured-description">
                        Step into the world of audio illusions. Can you recognize emotions from tone alone‚Äîwhen the
                        words try to fool you?
                        Go head-to-head against today‚Äôs smartest audio LLMs and see who really listens.
                    </p>

                    <div class="featured-stats">
                        <div class="stat-item">
                            <div class="stat-value">üéß 5</div>
                            <div class="stat-label">Audio Rounds</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-value">ü§ñ 3</div>
                            <div class="stat-label">AI Opponents</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-value">‚è± 2 min</div>
                            <div class="stat-label">Challenge Time</div>
                        </div>
                    </div>

                    <a href="game.html" class="btn-game-featured">
                        <i class="fas fa-play-circle"></i> Start Listening Challenge
                    </a>

                    <div class="hint-text">‚ö†Ô∏è Headphones recommended for the full experience</div>
                </div>

                <div class="featured-game-visual">
                    <div class="game-preview">
                        <div class="preview-icon">üîä</div>
                        <div class="preview-text dynamic-text">"I‚Äôm fine, really."</div>
                        <div class="preview-cues">
                            <div class="preview-cue lexical">üß† Words say: Calm</div>
                            <div class="preview-cue acoustic">üíî Voice says: Sad</div>
                        </div>
                        <div class="preview-question">Can <span class="highlight">you</span> tell the truth from the
                            tone?</div>
                        <div class="preview-button">
                            <button class="btn-listen">
                                ‚ñ∂ Listen & Guess
                            </button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="overview" class="section">
        <div class="container">
            <h2>Overview</h2>
            <div class="overview-content">
                <p>
                    <strong>LISTEN</strong> is a novel benchmark designed to evaluate multimodal audio-language models
                    on their ability to understand and distinguish between lexical and acoustic emotional cues in
                    speech.
                    The benchmark consists of four main experiment types:
                </p>
                <div class="experiment-cards">
                    <div class="card">
                        <div class="card-icon">1</div>
                        <h3>Neutral-Text</h3>
                        <p>Emotion recognition with neutral transcriptions across modalities</p>
                        <span class="badge">3 variants</span>
                    </div>
                    <div class="card">
                        <div class="card-icon">2</div>
                        <h3>Emotion-Matched</h3>
                        <p>Lexical and acoustic cues convey the same emotion</p>
                        <span class="badge">3 variants</span>
                    </div>
                    <div class="card">
                        <div class="card-icon">3</div>
                        <h3>Emotion-Mismatched</h3>
                        <p>Lexical and acoustic cues convey conflicting emotions</p>
                        <span class="badge">3 variants</span>
                    </div>
                    <div class="card">
                        <div class="card-icon">4</div>
                        <h3>Paralinguistic</h3>
                        <p>Non-verbal vocalizations without lexical content</p>
                        <span class="badge">1 variant</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="leaderboard" class="section section-gray">
        <div class="container">
            <h2>Leaderboard</h2>
            <p class="section-description">
                Performance of state-of-the-art audio-language models on the LISTEN benchmark.
                Click on column headers to sort.
            </p>

            <div class="leaderboard-controls">
                <div class="filter-group">
                    <label>Filter by Experiment:</label>
                    <select id="experimentFilter">
                        <option value="all">All Experiments</option>
                        <option value="exp1">Experiment 1 (Neutral-Text)</option>
                        <option value="exp2">Experiment 2 (Emotion-Matched)</option>
                        <option value="exp3">Experiment 3 (Emotion-Mismatched)</option>
                        <option value="exp4">Experiment 4 (Paralinguistic)</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Filter by Metric:</label>
                    <select id="metricFilter">
                        <option value="accuracy">Accuracy</option>
                        <option value="weighted_accuracy">Weighted Accuracy</option>
                        <option value="uar">UAR</option>
                        <option value="macro_f1">Macro F1</option>
                        <option value="micro_f1">Micro F1</option>
                    </select>
                </div>
                <div class="search-group">
                    <input type="text" id="modelSearch" placeholder="Search models...">
                </div>
            </div>

            <div class="table-container">
                <table id="leaderboardTable" class="leaderboard-table">
                    <thead>
                        <tr>
                            <th data-sort="rank">Rank</th>
                            <th data-sort="model">Model</th>
                            <th data-sort="type">Type</th>
                            <th data-sort="avg" class="sortable active">Overall Average <i class="fas fa-sort-down"></i>
                            </th>
                            <th data-sort="exp1_text">Neutral-Text (Text)</th>
                            <th data-sort="exp1_audio">Neutral-Text (Audio)</th>
                            <th data-sort="exp1_both">Neutral-Text (Both)</th>
                            <th data-sort="exp2_text">Emotion-Matched (Text)</th>
                            <th data-sort="exp2_audio">Emotion-Matched (Audio)</th>
                            <th data-sort="exp2_both">Emotion-Matched (Both)</th>
                            <th data-sort="exp3_text">Emotion-Mismatched (Text)</th>
                            <th data-sort="exp3_audio">Emotion-Mismatched (Audio)</th>
                            <th data-sort="exp3_both">Emotion-Mismatched (Both)</th>
                            <th data-sort="exp4_audio">Paralinguistic (Audio)</th>
                        </tr>
                    </thead>
                    <tbody id="leaderboardBody">
                        <!-- Data will be populated by JavaScript -->
                    </tbody>
                </table>
            </div>

            <div class="leaderboard-notes">
                <h3>Notes:</h3>
                <ul>
                    <li><strong>Overall Average</strong>: Mean accuracy across all audio and text+audio results from the
                        four experimental conditions (7 modalities total, excluding text-only)</li>
                    <li><strong>Weighted Accuracy</strong>: Accuracy weighted by class distribution</li>
                    <li><strong>UAR</strong>: Unweighted Average Recall (mean of per-class recalls)</li>
                    <li><strong>Macro F1</strong>: Unweighted mean of per-class F1 scores</li>
                    <li><strong>Micro F1</strong>: F1 score calculated globally across all classes</li>
                    <li><strong>Baseline Models</strong>: Uniform Guess and Majority Guess are not ranked with other
                        models</li>
                </ul>
            </div>
        </div>
    </section>

    <section id="visualizations" class="section">
        <div class="container">
            <h2>Performance Visualization</h2>
            <p class="section-description">
                Detailed model performance across different modalities and experimental conditions
            </p>

            <div class="visualization-single">
                <div class="viz-card-large">
                    <img src="hero_radar.png" alt="Detailed radar chart showing model performance across all modalities"
                        class="viz-image-large">
                    <p class="viz-caption">Comprehensive comparison of model performance across seven modalities:
                        Neutral-Text (Text/Audio), Emotion-Matched (Text/Audio), Emotion-Mismatched (Text/Audio), and
                        Paralinguistic (Audio). Gemini 2.5 Pro demonstrates the most balanced performance, while
                        Qwen3-Omni-30B excels in Emotion-Matched conditions.</p>
                </div>
            </div>
        </div>
    </section>



    <section id="experiments" class="section">
        <div class="container">
            <h2>Experiment Details</h2>
            <div class="experiment-details">
                <div class="detail-card">
                    <h3><span class="exp-number">1</span> Neutral-Text</h3>
                    <p><strong>Task:</strong> Emotion recognition with neutral transcriptions</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Text</code>: Neutral text transcription only</li>
                        <li><code>Audio</code>: Audio with emotional prosody</li>
                        <li><code>Text+Audio</code>: Both modalities (neutral text + emotional audio)</li>
                    </ul>
                    <p><strong>Purpose:</strong> Assess if models can recognize emotion from prosody when text is
                        neutral</p>
                </div>

                <div class="detail-card">
                    <h3><span class="exp-number">2</span> Emotion-Matched</h3>
                    <p><strong>Task:</strong> Emotion recognition when lexical and acoustic cues agree</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Text</code>: Emotional text only</li>
                        <li><code>Audio</code>: Audio with matching emotional prosody</li>
                        <li><code>Text+Audio</code>: Both modalities with matching emotions</li>
                    </ul>
                    <p><strong>Purpose:</strong> Baseline performance when both modalities provide consistent emotional
                        information</p>
                </div>

                <div class="detail-card">
                    <h3><span class="exp-number">3</span> Emotion-Mismatched</h3>
                    <p><strong>Task:</strong> Emotion recognition when lexical and acoustic cues conflict</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Text</code>: Emotional text (conflicting with audio emotion)</li>
                        <li><code>Audio</code>: Audio with conflicting emotional prosody</li>
                        <li><code>Text+Audio</code>: Both modalities with conflicting emotions</li>
                    </ul>
                    <p><strong>Purpose:</strong> Test whether models rely more on lexical or acoustic cues when they
                        conflict</p>
                </div>

                <div class="detail-card">
                    <h3><span class="exp-number">4</span> Paralinguistic</h3>
                    <p><strong>Task:</strong> Emotion recognition from non-verbal vocalizations</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Audio</code>: Non-verbal sounds (laughter, sighs, gasps, etc.)</li>
                    </ul>
                    <p><strong>Purpose:</strong> Evaluate understanding of purely acoustic emotional cues without
                        lexical content</p>
                </div>
            </div>
        </div>
    </section>

    <section id="examples" class="section section-gray">
        <div class="container">
            <h2>Condition Examples</h2>
            <p class="section-description">
                Representative examples from each experimental condition showing the task format and model predictions
            </p>

            <div class="examples-container">
                <!-- Neutral-Text Examples -->
                <div class="example-card">
                    <div class="example-header neutral-text">
                        <span class="example-icon">1</span>
                        <h3>Neutral-Text (Text-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_7c8b53fb</code>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"Kids are talking by the door."</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Read the transcription and classify the emotion. Based on the
                            content of this text, what emotion would the person likely be feeling?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. anger | B. fear | C. disgust | <span
                                class="highlight-answer">D. neutral</span> | E. sadness | F. surprise | G. calm | H.
                            happiness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth:</strong> <span class="badge-neutral">neutral</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">D (neutral) ‚úì</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The model correctly identifies the statement's emotion (neutral) from lexical cues alone.
                        </div>
                    </div>
                </div>

                <!-- Emotion-Matched Example -->
                <div class="example-card">
                    <div class="example-header emotion-matched">
                        <span class="example-icon">2</span>
                        <h3>Emotion-Matched (Text-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_9b76ea7d</code>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"What the hell is this?"</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Read the transcription and classify the emotion. From the semantic
                            content alone, what emotion is being expressed?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. neutral | B. sadness | C. excitement | D. frustration | E. fear
                            | F. disgust | G. happiness | <span class="highlight-answer">H. anger</span> | I. surprise
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth:</strong> <span class="badge-frustration">frustration</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">D (neutral) ‚úó</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The model misclassified an explicitly frustration utterance as neutral, illustrating
                            overgeneralization across semantically related negative emotions.
                        </div>
                    </div>
                </div>

                <!-- Emotion-Mismatched Text Example -->
                <div class="example-card">
                    <div class="example-header emotion-mismatched">
                        <span class="example-icon">3</span>
                        <h3>Emotion-Mismatched (Text-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_955399e0</code>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"You're right, the party's fantastic. Please, tell me
                                more. I haven't heard enough about it all week because hearing about that never gets
                                old!"</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Read the transcription and classify the emotion. What emotion is
                            conveyed by the words in this statement?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. surprise | <span class="highlight-answer">B. excitement</span>
                            | C. sadness | D. disgust | E. fear | F. neutral | G. anger | H. happiness | I. frustration
                            | J. ridicule
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth (Explicit):</strong> <span
                                    class="badge-excitement">excitement</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">B (excitement) ‚úì</span>
                            </div>
                            <div class="result-item">
                                <strong>Note:</strong> <span class="badge-warning">Audio conveys ridicule
                                    (conflict)</span>
                            </div>
                        </div>
                        <div class="example-note">
                            Although the lexical content expresses excitement, the corresponding audio conveys ridicule,
                            highlighting the designed lexical‚Äìprosodic conflict.
                        </div>
                    </div>
                </div>

                <!-- Emotion-Mismatched Audio Example -->
                <div class="example-card">
                    <div class="example-header emotion-mismatched">
                        <span class="example-icon">3</span>
                        <h3>Emotion-Mismatched (Audio-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_c52e71d0</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>MUStARD_PRO_1_7575_u_3B</code>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotion is
                            communicated through the speaker's vocal prosody?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. disgust | B. neutral | C. ridicule | D. frustration | E.
                            sadness | <span class="highlight-answer">F. anger</span> | G. excitement | H. fear | I.
                            surprise | J. happiness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth (Implicit):</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">G (excitement) ‚úó</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The lexical content is superficially positive, but the prosody expresses irritation and
                            anger. The model incorrectly predicts excitement, indicating difficulty in resolving
                            sarcastic or contrastive vocal tone.
                        </div>
                    </div>
                </div>

                <!-- Paralinguistic Example -->
                <div class="example-card">
                    <div class="example-header paralinguistic">
                        <span class="example-icon">4</span>
                        <h3>Paralinguistic (Audio-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_54df39ff</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>IEMOCAP_Session5_Ses05F_impro03_F006</code>
                        </div>
                        <div class="example-field">
                            <strong>Content:</strong> <em>Nonverbal laughter</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotional tone
                            is conveyed by the literal meaning of this statement?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. anger | B. happiness | C. fear | D. sadness | E. surprise | F.
                            frustration | <span class="highlight-answer">G. excitement</span> | H. disgust | I. neutral
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth:</strong> <span class="badge-excitement">excitement</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">B (happiness) ‚úó</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The utterance contains only nonverbal laughter. The model incorrectly classifies it as
                            happiness, revealing challenges in distinguishing subtle affective intent from nonverbal
                            vocalizations.
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="citation" class="section section-gray">
        <div class="container">
            <h2>Citation</h2>
            <p>If you use LISTEN in your research, please cite:</p>
            <div class="citation-box">
                <pre><code>@misc{deli2025listen,
  title={LISTEN: Lexical vs. Acoustic Emotion Benchmark for Audio Language Models},
  author={Deli, Jingyi C.},
  year={2025},
  publisher={GitHub},
  howpublished={\url{https://github.com/DeliJingyiC/LISTEN}},
  note={Dataset available at: \url{https://huggingface.co/datasets/delijingyic/VibeCheck}}
}</code></pre>
                <button class="copy-btn" onclick="copyBibtex()">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 LISTEN Benchmark. All rights reserved.</p>
            <p>
                <a href="https://github.com/DeliJingyiC/LISTEN" target="_blank">
                    <i class="fab fa-github"></i> GitHub
                </a>
                <a href="https://huggingface.co/datasets/delijingyic/VibeCheck" target="_blank">
                    <i class="fas fa-database"></i> Dataset
                </a>
            </p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>