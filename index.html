<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>LISTEN: Lexical vs. Acoustic Emotion Benchmark</title>

    <!-- Open Graph / LinkedIn Meta Tags -->
    <meta property="og:title" content="Do Audio LLMs LISTEN or just transcribe?">
    <meta property="og:description"
        content="A comprehensive benchmark revealing that most audio-language models over-rely on text and miss critical acoustic cues—especially when words and tone conflict.">
    <meta property="og:type" content="website">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Do Audio LLMs LISTEN or just transcribe?">
    <meta name="twitter:description"
        content="A comprehensive benchmark revealing that most audio-language models over-rely on text and miss critical acoustic cues—especially when words and tone conflict.">

    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>

<body>
    <nav class="navbar">
        <div class="container">
            <div class="nav-brand">
                <i class="fas fa-headphones"></i> LISTEN
            </div>
            <button class="mobile-menu-toggle" onclick="toggleMobileMenu()">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <div class="nav-links" id="navLinks">
                <button class="mobile-menu-close" onclick="toggleMobileMenu()">
                    <i class="fas fa-times"></i>
                </button>
                <a href="#overview">Overview</a>
                <a href="#leaderboard">Leaderboard</a>
                <a href="#visualizations">Results</a>
                <a href="#experiments">Experiments</a>
                <a href="#examples">Examples</a>
                <a href="#citation">Citation</a>
                <a href="https://github.com/DeliJingyiC/LISTEN" target="_blank">
                    <i class="fab fa-github"></i> GitHub
                </a>
            </div>
        </div>
    </nav>

    <header class="hero">
        <div class="container hero-container">
            <div class="hero-content">
                <h1>Do Audio LLMs <span class="flip-words"><span class="flip-word">LISTEN</span><span
                            class="flip-word">Transcribe</span></span>?</h1>
                <p class="subtitle">Measuring Lexical vs. Acoustic Emotion Cues Reliance in Audio LLMs</p>
                <p class="description">
                    A comprehensive benchmark revealing that most audio-language models over-rely on text and miss
                    critical acoustic cues—especially when words and tone conflict.
                </p>
                <div class="hero-buttons">
                    <a href="https://arxiv.org/abs/2510.10444" class="btn btn-primary" target="_blank">
                        <i class="fas fa-file-alt"></i> Read Paper
                    </a>
                    <a href="https://github.com/DeliJingyiC/LISTEN" class="btn btn-primary" target="_blank">
                        <i class="fab fa-github"></i> View on GitHub
                    </a>
                    <a href="https://huggingface.co/datasets/VibeCheck1/LISTEN_full" class="btn btn-secondary"
                        target="_blank">
                        <i class="fas fa-database"></i> Dataset
                    </a>
                    <a href="#citation" class="btn btn-secondary">
                        <i class="fas fa-quote-right"></i> Cite
                    </a>
                </div>
            </div>
            <div class="hero-visual">
                <img src="background_image4.png" alt="LISTEN Benchmark Illustration" class="hero-chart">
            </div>
        </div>
    </header>

    <style>
        .flip-card-container {
            perspective: 1000px;
            min-height: 500px;
            margin-bottom: 2rem;
        }

        .flip-card {
            position: relative;
            width: 100%;
            height: 100%;
            transition: transform 0.8s;
            transform-style: preserve-3d;
        }

        .flip-card.flipped {
            transform: rotateY(180deg);
        }

        .flip-card-front,
        .flip-card-back {
            position: absolute;
            width: 100%;
            height: 100%;
            backface-visibility: hidden;
            -webkit-backface-visibility: hidden;
            display: flex;
            flex-direction: column;
            padding: 0;
            margin: 0;
        }

        .flip-card-back {
            transform: rotateY(180deg);
        }

        .card-type-badge {
            display: inline-block;
            background: #F0F4FF;
            color: #1A1A1A;
            padding: 0.5rem 1rem;
            border-radius: 2rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            font-size: 0.9rem;
        }

        .question-content {
            background: #F8F7F5;
            padding: 2rem;
            border-radius: 1rem;
            margin-bottom: 1.5rem;
            border: 2px solid #E2E0DD;
            min-height: 150px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .question-text {
            font-size: 1.5rem;
            font-weight: 600;
            color: #1A1A1A;
            font-style: italic;
            text-align: center;
            line-height: 1.6;
        }

        .audio-player-embed {
            text-align: center;
            width: 100%;
        }

        .audio-icon-large {
            font-size: 3rem;
            color: #B08BBB;
            margin-bottom: 1rem;
            display: block;
        }

        .audio-control {
            width: 100%;
            max-width: 400px;
            margin: 1rem auto;
            display: block;
        }

        .audio-instruction {
            color: #6B6B6B;
            font-size: 0.95rem;
            margin-top: 0.5rem;
        }

        .question-prompt {
            font-size: 1.1rem;
            font-weight: 600;
            color: #1A1A1A;
            margin-bottom: 1.5rem;
            text-align: center;
            opacity: 1;
            visibility: visible;
        }

        .emotion-options {
            display: grid;
            grid-template-columns: repeat(5, 1fr);
            gap: 0.75rem;
            width: 100%;
            max-height: none;
            overflow: visible;
            grid-auto-rows: minmax(60px, auto);
        }

        .emotion-btn {
            background: #F8F7F5;
            border: 2px solid #E2E0DD;
            padding: 0.875rem;
            border-radius: 0.5rem;
            font-weight: 600;
            color: #1A1A1A;
            cursor: pointer;
            transition: all 0.3s ease;
            font-size: 0.95rem;
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
            word-wrap: break-word;
            white-space: normal;
            overflow: visible;
            text-overflow: unset;
        }

        .emotion-btn:hover {
            background: #F0F4FF;
            border-color: #B08BBB;
            transform: translateY(-2px);
        }

        .emotion-btn.selected {
            background: #2E2E2E;
            color: white;
            border-color: #2E2E2E;
        }

        .answer-feedback {
            animation: fadeIn 0.5s ease;
        }

        .feedback-content {
            background: #F8F7F5;
            padding: 2rem;
            border-radius: 1rem;
            border: 2px solid #E2E0DD;
        }

        .correct-answer-display {
            background: linear-gradient(135deg, #4E9A77, #3A7A5D);
            color: white;
            padding: 1.25rem;
            border-radius: 0.75rem;
            text-align: center;
            margin-bottom: 1.5rem;
        }

        .feedback-label {
            display: block;
            font-size: 0.9rem;
            opacity: 0.9;
            margin-bottom: 0.5rem;
        }

        .feedback-value {
            font-size: 1.5rem;
            font-weight: 700;
        }

        .ai-comparison h4 {
            color: #1A1A1A;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .ai-results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 0.75rem;
            margin-bottom: 1.5rem;
        }

        .ai-result-item {
            background: white;
            padding: 0.875rem;
            border-radius: 0.5rem;
            border-left: 4px solid #E2E0DD;
        }

        .ai-result-item.correct {
            border-left-color: #4E9A77;
            background: #F0FDF4;
        }

        .ai-result-item.incorrect {
            border-left-color: #B66161;
            background: #FEF2F2;
        }

        .ai-name-small {
            font-weight: 600;
            font-size: 0.85rem;
            color: #1A1A1A;
            margin-bottom: 0.25rem;
        }

        .ai-guess-small {
            font-size: 0.9rem;
            color: #6B6B6B;
        }

        .insight-message {
            background: #F0F4FF;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #B08BBB;
            color: #1A1A1A;
            line-height: 1.6;
            margin-bottom: 1.5rem;
        }

        .btn-next-round {
            background: #222222;
            color: white;
            border: 2px solid #222222;
            padding: 0.875rem 2rem;
            border-radius: 0.5rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            margin: 0 auto;
        }

        .btn-next-round:hover {
            background: transparent;
            color: #222222;
        }

        .final-results {
            animation: fadeIn 0.8s ease;
        }

        .results-content {
            text-align: center;
        }

        .trophy-display {
            width: 100px;
            height: 100px;
            background: #FEF9F0;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 1.5rem;
            border: 2px solid #E2E0DD;
        }

        .trophy-display i {
            font-size: 2.5rem;
            color: #C8A44B;
        }

        .results-content h3 {
            font-size: 2rem;
            color: #1A1A1A;
            margin-bottom: 2rem;
        }

        .score-display {
            margin-bottom: 2rem;
        }

        .user-score {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
        }

        .score-label {
            font-size: 1rem;
            color: #6B6B6B;
        }

        .score-number {
            font-size: 3rem;
            font-weight: 700;
            color: #B08BBB;
        }

        .ai-scores-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
            gap: 1rem;
            margin-bottom: 2rem;
        }

        .ai-score-card {
            background: #F8F7F5;
            padding: 1rem;
            border-radius: 0.5rem;
            border: 1px solid #E2E0DD;
        }

        .ai-score-card.winner {
            border: 2px solid #C8A44B;
            background: #FEF9F0;
        }

        .ai-model-name {
            font-size: 0.85rem;
            color: #6B6B6B;
            margin-bottom: 0.5rem;
        }

        .ai-score-value {
            font-size: 1.5rem;
            font-weight: 700;
            color: #1A1A1A;
        }

        .performance-breakdown {
            background: #F8F7F5;
            padding: 1.5rem;
            border-radius: 0.75rem;
            margin-bottom: 1.5rem;
        }

        .performance-breakdown h4 {
            color: #1A1A1A;
            margin-bottom: 1rem;
        }

        .perf-bars {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .perf-bar {
            display: grid;
            grid-template-columns: 140px 1fr 60px;
            align-items: center;
            gap: 1rem;
        }

        .perf-label {
            font-weight: 600;
            color: #1A1A1A;
            font-size: 0.9rem;
        }

        .perf-bar-container {
            background: white;
            height: 32px;
            border-radius: 0.5rem;
            overflow: hidden;
            border: 1px solid #E2E0DD;
        }

        .perf-bar-fill {
            background: linear-gradient(90deg, #2E2E2E, #4A4A4A);
            height: 100%;
            transition: width 1s ease;
        }

        .perf-bar-fill.acoustic {
            background: linear-gradient(90deg, #B08BBB, #C7B8EA);
        }

        .perf-percent {
            font-weight: 600;
            color: #1A1A1A;
            text-align: right;
        }

        .results-message {
            background: #FEF9F0;
            padding: 1.5rem;
            border-radius: 0.75rem;
            border: 2px solid #C8A44B;
            margin-bottom: 1.5rem;
            line-height: 1.7;
            color: #1A1A1A;
        }

        .results-actions {
            display: flex;
            gap: 1rem;
            justify-content: center;
            flex-wrap: wrap;
        }

        .btn-restart,
        .btn-view-full {
            padding: 0.875rem 1.75rem;
            border-radius: 0.5rem;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            text-decoration: none;
        }

        .btn-restart {
            background: #222222;
            color: white;
            border: 2px solid #222222;
        }

        .btn-restart:hover {
            background: transparent;
            color: #222222;
        }

        .btn-view-full {
            background: transparent;
            color: #222222;
            border: 2px solid #E2E0DD;
        }

        .btn-view-full:hover {
            border-color: #222222;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(20px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .audio-player-example {
            margin-top: 10px;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 8px;
            display: flex;
            justify-content: center;
        }

        .audio-player-example audio {
            width: 100%;
            max-width: 400px;
            height: 40px;
            outline: none;
        }

        @media (max-width: 768px) {
            .emotion-emoji-btn {
                font-size: 2em;
                min-width: 80px;
                padding: 15px;
            }

            .comparison-row {
                grid-template-columns: 1fr;
            }

            .text-display h2 {
                font-size: 1.3em;
            }

            .game-embed {
                padding: 1.5rem;
            }

            .audio-player-example audio {
                max-width: 100%;
            }
        }
    </style>


    <section id="overview" class="section">
        <div class="container">
            <h2>Overview</h2>
            <div class="overview-content">
                <p>
                    <strong>LISTEN</strong> is a novel benchmark designed to evaluate multimodal audio-language models
                    on their ability to understand and distinguish between lexical and acoustic emotional cues in
                    speech.
                    The benchmark consists of four main experiment types:
                </p>
                <div class="experiment-cards">
                    <div class="card">
                        <div class="card-icon">1</div>
                        <h3>Neutral-Text</h3>
                        <p>Emotion recognition with neutral transcriptions across modalities</p>
                        <span class="badge">3 variants</span>
                    </div>
                    <div class="card">
                        <div class="card-icon">2</div>
                        <h3>Emotion-Matched</h3>
                        <p>Lexical and acoustic cues convey the same emotion</p>
                        <span class="badge">3 variants</span>
                    </div>
                    <div class="card">
                        <div class="card-icon">3</div>
                        <h3>Emotion-Mismatched</h3>
                        <p>Lexical and acoustic cues convey conflicting emotions</p>
                        <span class="badge">3 variants</span>
                    </div>
                    <div class="card">
                        <div class="card-icon">4</div>
                        <h3>Paralinguistic</h3>
                        <p>Non-verbal vocalizations without lexical content</p>
                        <span class="badge">1 variant</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="leaderboard" class="section section-gray">
        <div class="container">
            <h2>Leaderboard</h2>
            <p class="section-description">
                Performance of state-of-the-art audio-language models on the LISTEN benchmark.
                Click on column headers to sort.
            </p>

            <div class="leaderboard-controls">
                <div class="filter-group">
                    <label>Filter by Experiment:</label>
                    <select id="experimentFilter">
                        <option value="all">All Experiments</option>
                        <option value="exp1">Experiment 1 (Neutral-Text)</option>
                        <option value="exp2">Experiment 2 (Emotion-Matched)</option>
                        <option value="exp3">Experiment 3 (Emotion-Mismatched)</option>
                        <option value="exp4">Experiment 4 (Paralinguistic)</option>
                    </select>
                </div>
                <div class="filter-group">
                    <label>Filter by Metric:</label>
                    <select id="metricFilter">
                        <option value="accuracy">Accuracy</option>
                        <option value="weighted_accuracy">Weighted Accuracy</option>
                        <option value="uar">UAR</option>
                        <option value="macro_f1">Macro F1</option>
                        <option value="micro_f1">Micro F1</option>
                    </select>
                </div>
                <div class="search-group">
                    <input type="text" id="modelSearch" placeholder="Search models...">
                </div>
            </div>

            <div class="table-container">
                <table id="leaderboardTable" class="leaderboard-table">
                    <thead>
                        <tr>
                            <th data-sort="rank">Rank</th>
                            <th data-sort="model">Model</th>
                            <th data-sort="type">Type</th>
                            <th data-sort="avg" class="sortable active">Overall Average <i class="fas fa-sort-down"></i>
                            </th>
                            <th data-sort="exp1_text">Neutral-Text (Text)</th>
                            <th data-sort="exp1_audio">Neutral-Text (Audio)</th>
                            <th data-sort="exp1_both">Neutral-Text (Both)</th>
                            <th data-sort="exp2_text">Emotion-Matched (Text)</th>
                            <th data-sort="exp2_audio">Emotion-Matched (Audio)</th>
                            <th data-sort="exp2_both">Emotion-Matched (Both)</th>
                            <th data-sort="exp3_text">Emotion-Mismatched (Text)</th>
                            <th data-sort="exp3_audio">Emotion-Mismatched (Audio)</th>
                            <th data-sort="exp3_both">Emotion-Mismatched (Both)</th>
                            <th data-sort="exp4_audio">Paralinguistic (Audio)</th>
                        </tr>
                    </thead>
                    <tbody id="leaderboardBody">
                        <!-- Data will be populated by JavaScript -->
                    </tbody>
                </table>
            </div>

            <div class="leaderboard-notes">
                <h3>Notes:</h3>
                <ul>
                    <li><strong>Overall Average</strong>: Mean accuracy across all audio and text+audio results from the
                        four experimental conditions (7 modalities total, excluding text-only)</li>
                    <li><strong>Weighted Accuracy</strong>: Accuracy weighted by class distribution</li>
                    <li><strong>UAR</strong>: Unweighted Average Recall (mean of per-class recalls)</li>
                    <li><strong>Macro F1</strong>: Unweighted mean of per-class F1 scores</li>
                    <li><strong>Micro F1</strong>: F1 score calculated globally across all classes</li>
                    <li><strong>Baseline Models</strong>: Uniform Guess and Majority Guess are not ranked with other
                        models</li>
                </ul>
            </div>
        </div>
    </section>

    <section id="visualizations" class="section">
        <div class="container">
            <h2>Performance Visualization</h2>
            <p class="section-description">
                Detailed model performance across different modalities and experimental conditions
            </p>

            <div class="visualization-single">
                <div class="viz-card-large">
                    <img src="hero_radar.png" alt="Detailed radar chart showing model performance across all modalities"
                        class="viz-image-large">
                    <p class="viz-caption">Comprehensive comparison of model performance across seven modalities:
                        Neutral-Text (Text/Audio), Emotion-Matched (Text/Audio), Emotion-Mismatched (Text/Audio), and
                        Paralinguistic (Audio). Gemini 2.5 Pro demonstrates the most balanced performance, while
                        Qwen3-Omni-30B excels in Emotion-Matched conditions.</p>
                </div>
            </div>
        </div>
    </section>



    <section id="experiments" class="section">
        <div class="container">
            <h2>Experiment Details</h2>
            <div class="experiment-details">
                <div class="detail-card">
                    <h3><span class="exp-number">1</span> Neutral-Text</h3>
                    <p><strong>Task:</strong> Emotion recognition with neutral transcriptions</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Text</code>: Neutral text transcription only</li>
                        <li><code>Audio</code>: Audio with emotional prosody</li>
                        <li><code>Text+Audio</code>: Both modalities (neutral text + emotional audio)</li>
                    </ul>
                    <p><strong>Purpose:</strong> Assess if models can recognize emotion from prosody when text is
                        neutral</p>
                </div>

                <div class="detail-card">
                    <h3><span class="exp-number">2</span> Emotion-Matched</h3>
                    <p><strong>Task:</strong> Emotion recognition when lexical and acoustic cues agree</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Text</code>: Emotional text only</li>
                        <li><code>Audio</code>: Audio with matching emotional prosody</li>
                        <li><code>Text+Audio</code>: Both modalities with matching emotions</li>
                    </ul>
                    <p><strong>Purpose:</strong> Baseline performance when both modalities provide consistent emotional
                        information</p>
                </div>

                <div class="detail-card">
                    <h3><span class="exp-number">3</span> Emotion-Mismatched</h3>
                    <p><strong>Task:</strong> Emotion recognition when lexical and acoustic cues conflict</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Text</code>: Emotional text (conflicting with audio emotion)</li>
                        <li><code>Audio</code>: Audio with conflicting emotional prosody</li>
                        <li><code>Text+Audio</code>: Both modalities with conflicting emotions</li>
                    </ul>
                    <p><strong>Purpose:</strong> Test whether models rely more on lexical or acoustic cues when they
                        conflict</p>
                </div>

                <div class="detail-card">
                    <h3><span class="exp-number">4</span> Paralinguistic</h3>
                    <p><strong>Task:</strong> Emotion recognition from non-verbal vocalizations</p>
                    <p><strong>Variants:</strong></p>
                    <ul>
                        <li><code>Audio</code>: Non-verbal sounds (laughter, sighs, gasps, etc.)</li>
                    </ul>
                    <p><strong>Purpose:</strong> Evaluate understanding of purely acoustic emotional cues without
                        lexical content</p>
                </div>
            </div>
        </div>
    </section>

    <section id="examples" class="section section-gray">
        <div class="container">
            <h2>Condition Examples</h2>
            <p class="section-description">
                Representative examples from each experimental condition showing the task format and model predictions
            </p>

            <div class="examples-container">
                <!-- Neutral-Text Examples -->
                <div class="example-card">
                    <div class="example-header neutral-text">
                        <span class="example-icon">1</span>
                        <h3>Neutral-Text (Text-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_7c8b53fb</code>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"It's elevn o'clock."</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Read the transcription and classify the emotion. Based on the
                            content of this text, what emotion would the person likely be feeling?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. anger | B. fear | C. disgust | D. neutral | E. sadness | F.
                            surprise | G. calm | H.
                            happiness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth:</strong> <span class="badge-neutral">neutral</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">D (neutral) ✓</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The model correctly identifies the statement's emotion (neutral) from lexical cues alone.
                        </div>
                    </div>
                </div>

                <!-- Emotion-Matched Example -->
                <div class="example-card">
                    <div class="example-header emotion-matched">
                        <span class="example-icon">2</span>
                        <h3>Emotion-Matched (Text-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_9b76ea7d</code>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"You are cruising for a bruising. You are in so much
                                trouble."</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Read the transcription and classify the emotion. From the semantic
                            content alone, what emotion is being expressed?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. neutral | B. sadness | C. excitement |D. frustration | E. fear
                            | F. disgust | G. happiness | H. anger | I. surprise
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth:</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">A (neutral) ✗</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The model misclassified an explicitly angry utterance as neutral, illustrating
                            overgeneralization across semantically related negative emotions.
                        </div>
                    </div>
                </div>

                <!-- Emotion-Mismatched Text Example -->
                <div class="example-card">
                    <div class="example-header emotion-mismatched">
                        <span class="example-icon">3</span>
                        <h3>Emotion-Mismatched (Text-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_955399e0</code>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"You're right, the party's fantastic. Please, tell me
                                more. I haven't heard enough about it all week because hearing about that never gets
                                old!"</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Read the transcription and classify the emotion. What emotion is
                            conveyed by the words in this statement?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. surprise | B. excitement
                            | C. sadness | D. disgust | E. fear | F. neutral | G. anger | H. happiness | I. frustration
                            | J. ridicule
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth (Explicit):</strong> <span
                                    class="badge-excitement">excitement</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">B (excitement) ✓</span>
                            </div>

                        </div>

                    </div>
                </div>
                <!-- Neutral-Text Audio Example -->
                <div class="example-card">
                    <div class="example-header neutral-text">
                        <span class="example-icon">4</span>
                        <h3>Neutral-Text (Audio-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_7c8b53fb</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>CREMAD_train_0333</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="1011_IEO_ANG_HI.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotion is
                            communicated through the speaker's vocal prosody?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. surprise | B. sadness | C. fear | D. anger | E.
                            calm |F. happiness | G. neutral | H. disgust
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth :</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">D (anger) ✓</span>
                            </div>
                        </div>

                    </div>
                </div>
                <!-- Emotion-Matched Audio Example -->
                <div class="example-card">
                    <div class="example-header emotion-matched">
                        <span class="example-icon">5</span>
                        <h3>Emotion-Matched (Audio-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_dd0f6e9d</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>IEMOCAP_Session5_Ses05M_script01_1b_F030</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="Ses01F_impro05_F021.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotion is
                            communicated through the speaker's vocal prosody?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. frustration | B. anger | C. neutral | D. excitement | E.
                            happiness |F. surprise | G. disgust | H. fear | I. sadness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth :</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">B (anger) ✓</span>
                            </div>
                        </div>

                    </div>
                </div>
                <!-- Emotion-Mismatched Audio Example -->
                <div class="example-card">
                    <div class="example-header emotion-mismatched">
                        <span class="example-icon">6</span>
                        <h3>Emotion-Mismatched (Audio-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_c52e71d0</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>MUStARD_PRO_1_7575_u_3B</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="1_7575_u.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotion is
                            communicated through the speaker's vocal prosody?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. disgust | B. neutral | C. ridicule | D. frustration | E.
                            sadness | F. anger | G. excitement | H. fear | I.
                            surprise | J. happiness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth (Implicit):</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">G (excitement) ✗</span>
                            </div>
                        </div>

                    </div>
                </div>
                <!-- Neutral-Text Audio Example -->
                <div class="example-card">
                    <div class="example-header neutral-text">
                        <span class="example-icon">7</span>
                        <h3>Neutral-Text (Text+Audio)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_7c8b53fb</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>CREMAD_train_0333</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="1011_IEO_ANG_HI.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"It's elevn o'clock."</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and read the transcription, then classify the
                            emotion. What emotion does the speaker convey through their tone?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. surprise | B. sadness | C. fear | D. anger | E.
                            calm |F. happiness | G. neutral | H. disgust
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth :</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">D (anger) ✓</span>
                            </div>
                        </div>

                    </div>
                </div>
                <!-- Emotion-Matched Audio Example -->
                <div class="example-card">
                    <div class="example-header emotion-matched">
                        <span class="example-icon">8</span>
                        <h3>Emotion-Matched (Text+Audio)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_dd0f6e9d</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>IEMOCAP_Session5_Ses05M_script01_1b_F030</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="Ses01F_impro05_F021.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"You are cruising for a bruising. You are in so much
                                trouble."</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotion is
                            communicated through the speaker's vocal prosody?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. frustration | B. anger | C. neutral | D. excitement | E.
                            happiness |F. surprise | G. disgust | H. fear | I. sadness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth :</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-correct">B (anger) ✓ </span>
                            </div>
                        </div>

                    </div>
                </div>
                <!-- Emotion-Mismatched Audio Example -->
                <div class="example-card">
                    <div class="example-header emotion-mismatched">
                        <span class="example-icon">9</span>
                        <h3>Emotion-Mismatched (Text+Audio)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_c52e71d0</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>MUStARD_PRO_1_7575_u_3B</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="1_7575_u.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Transcription:</strong> <em>"You're right, the party's fantastic. Please, tell me
                                more. I haven't heard enough about it all week because hearing about that never gets
                                old!"</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotion is
                            communicated through the speaker's vocal prosody?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. disgust | B. neutral | C. ridicule | D. frustration | E.
                            sadness | F. anger | G. excitement | H. fear | I.
                            surprise | J. happiness
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth (Implicit):</strong> <span class="badge-anger">anger</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">G (excitement) ✗</span>
                            </div>
                        </div>

                    </div>
                </div>

                <!-- Paralinguistic Example -->
                <div class="example-card">
                    <div class="example-header paralinguistic">
                        <span class="example-icon">10</span>
                        <h3>Paralinguistic (Audio-only)</h3>
                    </div>
                    <div class="example-content">
                        <div class="example-field">
                            <strong>Sample ID:</strong> <code>SAMPLE_54df39ff</code>
                        </div>
                        <div class="example-field">
                            <strong>Audio:</strong> <code>IEMOCAP_Ses01F_script01_3_F012</code>
                            <div class="audio-player-example">
                                <audio controls>
                                    <source src="Ses01F_script01_3_F012.wav" type="audio/wav">
                                    Your browser does not support the audio element.
                                </audio>
                            </div>
                        </div>
                        <div class="example-field">
                            <strong>Content:</strong> <em>Nonverbal laughter</em>
                        </div>
                        <div class="example-field">
                            <strong>Prompt:</strong> Listen to the audio and classify the emotion. What emotional tone
                            is conveyed by the literal meaning of this statement?
                        </div>
                        <div class="example-choices">
                            <strong>Choices:</strong> A. anger | B. happiness | C. fear | D. sadness | E. surprise | F.
                            frustration | G. excitement | H. disgust | I. neutral
                        </div>
                        <div class="example-results">
                            <div class="result-item">
                                <strong>Ground Truth:</strong> <span class="badge-happiness">happiness</span>
                            </div>
                            <div class="result-item">
                                <strong>Model Prediction:</strong> <span class="badge-incorrect">B (happiness) ✓</span>
                            </div>
                        </div>
                        <div class="example-note">
                            The utterance contains only nonverbal sighs. The model correctly classifies it as
                            happiness.
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="citation" class="section section-gray">
        <div class="container">
            <h2>Citation</h2>
            <p>If you use LISTEN in your research, please cite:</p>
            <div class="citation-box">
                <pre><code>@misc{chen2025audiollmsreallylisten,
                    title={Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance}, 
                    author={Jingyi Chen and Zhimeng Guo and Jiyun Chun and Pichao Wang and Andrew Perrault and Micha Elsner},
                    year={2025},
                    eprint={2510.10444},
                    archivePrefix={arXiv},
                    primaryClass={cs.CL},
                    url={https://arxiv.org/abs/2510.10444}, 
              }</code></pre>
                <button class="copy-btn" onclick="copyBibtex()">
                    <i class="fas fa-copy"></i> Copy
                </button>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2025 LISTEN Benchmark. All rights reserved.</p>
            <p>
                <a href="https://arxiv.org/abs/2510.10444" target="_blank">
                    <i class="fas fa-file-alt"></i> Paper
                </a>
                <a href="https://github.com/DeliJingyiC/LISTEN" target="_blank">
                    <i class="fab fa-github"></i> GitHub
                </a>
                <a href="https://huggingface.co/datasets/delijingyic/VibeCheck" target="_blank">
                    <i class="fas fa-database"></i> Dataset
                </a>
            </p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>

</html>

</html>